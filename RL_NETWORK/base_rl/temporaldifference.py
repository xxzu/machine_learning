#tdæ—¶åºå·®åˆ†ç®—æ³•
#V(s)â†V(s)+Î±â‹…Î´
#Î´tâ€‹=Rt+1 +Î³V(sâ€²)âˆ’V(s) 
#ä»çŠ¶æ€ ğ‘  åˆ°ä¸‹ä¸€çŠ¶æ€ ğ‘ â€²è·å¾—çš„å³æ—¶å¥–åŠ±ã€‚
# V(sâ€²) ä¸‹ä¸€çŠ¶æ€s'çš„ä»·å€¼ä¼°è®¡


# åˆå§‹åŒ–çŠ¶æ€ä»·å€¼
values = {"A": 0, "B": 0, "C": 0}  # çŠ¶æ€çš„åˆå§‹ä»·å€¼
alpha = 0.1  # å­¦ä¹ ç‡
gamma = 0.9  # æŠ˜æ‰£å› å­

# ä¸€ä¸ªå›åˆçš„è½¨è¿¹
trajectory = [("A", 1, "B"), ("B", 2, "C"), ("C", 0, None)]
# è½¨è¿¹æ ¼å¼ï¼š(å½“å‰çŠ¶æ€, å¥–åŠ±, ä¸‹ä¸€çŠ¶æ€)

# æ›´æ–°çŠ¶æ€ä»·å€¼
for state, reward, next_state in trajectory:
    if next_state is None:
        td_target = reward  # ç»ˆæ­¢çŠ¶æ€çš„ç›®æ ‡åªåŒ…å«å³æ—¶å¥–åŠ±
    else:
        td_target = reward + gamma * values[next_state]  # TDç›®æ ‡  Rt+1 +Î³V(sâ€²)

    td_error = td_target - values[state]  # è®¡ç®—TDè¯¯å·® Î´tâ€‹=Rt+1 +Î³V(sâ€²)âˆ’V(s) 
    values[state] += alpha * td_error  # æ›´æ–°ä»·å€¼å‡½æ•° #V(s)â†V(s)+Î±â‹…Î´

# è¾“å‡ºæœ€ç»ˆçš„çŠ¶æ€ä»·å€¼
print("çŠ¶æ€ä»·å€¼ä¼°è®¡ï¼š", values)
