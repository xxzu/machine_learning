'''DPG çš„æ ¸å¿ƒæ€æƒ³
ç­–ç•¥ç½‘ç»œï¼ˆDeterministic Policyï¼‰ï¼šç¡®å®šæ€§ç­–ç•¥æ„å‘³ç€ï¼Œç»™å®šä¸€ä¸ªçŠ¶æ€ 
ğ‘ ï¼Œç­–ç•¥ç½‘ç»œ ğœ‹ğœƒ(ğ‘ )  ä¼šç›´æ¥è¾“å‡ºä¸€ä¸ª ç¡®å®šçš„åŠ¨ä½œa =ğœ‹ğœƒ(ğ‘ )è€Œä¸æ˜¯åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒã€‚

ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼šDPGè®¡ç®—ç­–ç•¥æ¢¯åº¦ï¼Œä½†æ˜¯ä¸ä¼ ç»Ÿçš„æ¦‚ç‡ç­–ç•¥æ¢¯åº¦ä¸åŒï¼ŒDPG è®¡ç®—çš„æ˜¯ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ã€‚å®ƒé€šè¿‡Qç½‘ç»œ ä¼°è®¡ç­–ç•¥çš„åŠ¨ä½œå€¼ï¼Œ
å¹¶ä¸”é€šè¿‡è®¡ç®—ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦æ¥æ›´æ–°ç½‘ç»œã€‚

ç›®æ ‡ï¼šDPG çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–é•¿æœŸå›æŠ¥ã€‚å®ƒä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼Œé€šè¿‡ Q ç½‘ç»œ æ¥è®¡ç®—æ¢¯åº¦ï¼Œå¹¶ä¸”é€šè¿‡ ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ æ¥æ›´æ–°å‚æ•°ã€‚
'''
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import gym

# Qç½‘ç»œ
class QNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_size)
    
    def forward(self, state):
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# ç­–ç•¥ç½‘ç»œ
class ActorNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(ActorNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_size)
    
    def forward(self, state):
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x  # Deterministic action

# DPG Agent
class DPGAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.q_network = QNetwork(state_size, action_size)
        self.actor = ActorNetwork(state_size, action_size)
        self.target_q_network = QNetwork(state_size, action_size)
        self.target_q_network.load_state_dict(self.q_network.state_dict())
        self.optimizer = optim.Adam(list(self.q_network.parameters()) + list(self.actor.parameters()), lr=0.001)
        
    def act(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)  # Add batch dimension
        with torch.no_grad():
            action_values = self.actor(state)  # Deterministic action
        action = torch.argmax(action_values, dim=1).item() 
        return action

    # def update(self, state, action, reward, next_state, done):
    #     state = torch.FloatTensor(state).unsqueeze(0)
    #     next_state = torch.FloatTensor(next_state).unsqueeze(0)
        
    #     # Compute Q value using Q network
    #     q_value = self.q_network(state)[0, action]
    #     # Compute target Q value using target Q network
    #     target_q_value = reward + (1 - done) * self.target_q_network(next_state).max()

    #     # Update Q network (Bellman error)
    #     q_loss = nn.MSELoss()(q_value, target_q_value)
    #     self.optimizer.zero_grad()
    #     q_loss.backward()
    #     self.optimizer.step()

    #     # Update Actor network based on Q network's gradient
    #     # This part typically requires applying the policy gradient
    #     # Since here the actor network is trained through deterministic gradient descent
    #     self.optimizer.zero_grad()
    #     action_grad = torch.autograd.grad(q_value, self.actor.parameters())
    #     for param, grad in zip(self.actor.parameters(), action_grad):
    #         param.grad = grad
    #     self.optimizer.step()
    def update(self, state, action, reward, next_state, done):
        state = torch.FloatTensor(state).unsqueeze(0)
        next_state = torch.FloatTensor(next_state).unsqueeze(0)
        
        # Compute Q value using Q network
        q_value = self.q_network(state)[0, action]
        # Compute target Q value using target Q network
        target_q_value = reward + (1 - done) * self.target_q_network(next_state).max()

        # Update Q network (Bellman error)
        q_loss = nn.MSELoss()(q_value, target_q_value)
        self.optimizer.zero_grad()
        q_loss.backward()
        self.optimizer.step()

        # Update Actor network based on Q network's gradient
        # Now we compute the gradient of Q with respect to the action, and backpropagate it to the Actor network.
        action_value = self.actor(state)
        action_grad = torch.autograd.grad(action_value, self.actor.parameters(), grad_outputs=torch.ones_like(action_value))
        
        # Apply the policy gradient step
        for param, grad in zip(self.actor.parameters(), action_grad):
            param.grad = grad
        self.optimizer.step()

if __name__ == "__main__":
    env = gym.make("CartPole-v1", render_mode='human')
    
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n
    agent = DPGAgent(state_size, action_size)

    episodes = 500
    for episode in range(episodes):
        state, _ = env.reset()
        total_reward = 0

        while True:
            # Get action from agent
            action = agent.act(state)
            # Execute action in the environment
            next_state, reward, terminated, truncated, _ = env.step(action)
            total_reward += reward

            # Update the agent (Q network and actor network)
            agent.update(state, action, reward, next_state, terminated or truncated)
            
            state = next_state

            if terminated or truncated:
                break
        
        print(f"Episode: {episode+1}, Total Reward: {total_reward}")
    torch.save(agent.actor.state_dict(), "/home/ubuntu/machine_learning/policy_base/dpg_actor.pth")
    env.close()
